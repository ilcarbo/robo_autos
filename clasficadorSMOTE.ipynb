{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clasifición de recuperados y no recuperados utilizando SMOTE\n",
    "\n",
    "A continuación vamos a balancear nuestro dataset (cantidad de autos recuperados vs. no recuperados) subsampleando la clase mayoritaria (autos no recuperados ~95%) y sobresampleando la clase minoritaria (autos recuperados ~5%).\n",
    "\n",
    "## Preparación y caracterización del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos las librerías\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos los datos y dropeamos NaNs y una columna que quedó de más\n",
    "data_total = pd.read_csv('./curados/ULTIMATE_DATASET.csv')\n",
    "data_total.dropna(inplace=True) # Dropeamos NaNs\n",
    "data_total.drop(['Unnamed: 0'], axis=1, inplace=True) # Dropeamos una columna extra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hay 59986 no recuperados y 2925 recuperados\n",
      "El porcentaje de recuperados en el dataset es 4.9 %\n"
     ]
    }
   ],
   "source": [
    "# Vemos cuántas denuncias de robo hay (es decir, cuántos no recuperados) y cuántas notificaciones\n",
    "# de recupero hay (es decir, recuperados)\n",
    "print(\"Hay \"+ str(data_total['tramite_tipo'].value_counts()[0])+ ' no recuperados y '+\n",
    "              str(data_total['tramite_tipo'].value_counts()[1])+ ' recuperados')\n",
    "print(\"El porcentaje de recuperados en el dataset es \"+ \n",
    "      str(round(data_total['tramite_tipo'].value_counts()[1]/data_total['tramite_tipo'].value_counts()[0],3)*100)+ ' %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cambio los valores de la columna tramite_tipo apra que robado sea 0 y recuperado 1\n",
    "data_total['tramite_tipo'] = data_total['tramite_tipo'].map({'DENUNCIA DE ROBO O HURTO': 0, \n",
    "                                                             'COMUNICACIÓN DE RECUPERO': 1})\n",
    "\n",
    "# Y cambiamos el nombre de la columna a \"recuperados\" con 0 cuando no están recuperados y 1 cuando\n",
    "# si fueron recuperados\n",
    "data_total = data_total.rename(columns={'tramite_tipo':'recuperados'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['recuperados', 'tramite_fecha', 'fecha_inscripcion_inicial',\n",
       "       'registro_seccional_descripcion', 'registro_seccional_provincia',\n",
       "       'automotor_origen', 'automotor_anio_modelo',\n",
       "       'automotor_tipo_descripcion', 'automotor_marca_descripcion',\n",
       "       'automotor_modelo_descripcion', 'automotor_uso_descripcion',\n",
       "       'titular_tipo_persona', 'titular_domicilio_localidad',\n",
       "       'titular_domicilio_provincia', 'titular_genero',\n",
       "       'titular_anio_nacimiento', 'titular_pais_nacimiento', 'unico_duenio',\n",
       "       'dia_anio'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List de las columnas\n",
    "data_total.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quitamos los que tienen Uso no declarado\n",
    "data_total = data_total.loc[data_total['automotor_uso_descripcion']!='No declarado']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropeamos las columnas 'registro_seccional_descripcion' y 'titular_domicilio_localidad' porque\n",
    "# no las consideramos relevantes para la prediccioón\n",
    "data_total.drop(['registro_seccional_descripcion', 'titular_domicilio_localidad',], \n",
    "                axis= 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertimos las fechas a datetime64\n",
    "data_total['tramite_fecha']             = pd.to_datetime(data_total['tramite_fecha'])\n",
    "data_total['fecha_inscripcion_inicial'] = pd.to_datetime(data_total['fecha_inscripcion_inicial'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos columnas con el día, mes y año de robo más una columna con el año de patentamiento\n",
    "data_total['dia_robo'] = data_total['tramite_fecha'].apply(lambda x: int(x.isoweekday())) #lunes es 1, domingo 7\n",
    "data_total['mes_robo'] = data_total['tramite_fecha'].apply(lambda x: int(x.month))\n",
    "data_total['dia_del_anio'] = data_total['tramite_fecha'].apply(lambda x: int(x.timetuple().tm_yday))\n",
    "data_total['anio_pat'] = data_total['fecha_inscripcion_inicial'].apply(lambda x: int(x.year))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Una nueva columna que dice si el titular está radicado en la misma provincia que el auto\n",
    "data_total['tit_radicado'] = ((data_total['registro_seccional_provincia']).apply(lambda x: x.upper()) == data_total['titular_domicilio_provincia']).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>recuperados</th>\n",
       "      <th>registro_seccional_provincia</th>\n",
       "      <th>automotor_origen</th>\n",
       "      <th>automotor_anio_modelo</th>\n",
       "      <th>automotor_tipo_descripcion</th>\n",
       "      <th>automotor_marca_descripcion</th>\n",
       "      <th>automotor_modelo_descripcion</th>\n",
       "      <th>automotor_uso_descripcion</th>\n",
       "      <th>titular_tipo_persona</th>\n",
       "      <th>titular_genero</th>\n",
       "      <th>titular_anio_nacimiento</th>\n",
       "      <th>titular_pais_nacimiento</th>\n",
       "      <th>unico_duenio</th>\n",
       "      <th>dia_anio</th>\n",
       "      <th>dia_robo</th>\n",
       "      <th>mes_robo</th>\n",
       "      <th>dia_del_anio</th>\n",
       "      <th>anio_pat</th>\n",
       "      <th>tit_radicado</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Buenos Aires</td>\n",
       "      <td>Nacional</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>SEDAN 4 P</td>\n",
       "      <td>CHEVROLET</td>\n",
       "      <td>CORSA</td>\n",
       "      <td>Privado</td>\n",
       "      <td>Física</td>\n",
       "      <td>Masculino</td>\n",
       "      <td>1981.0</td>\n",
       "      <td>ARGENTINA</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>2000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Buenos Aires</td>\n",
       "      <td>Nacional</td>\n",
       "      <td>2007.0</td>\n",
       "      <td>FURGON</td>\n",
       "      <td>PEUGEOT</td>\n",
       "      <td>PARTNER</td>\n",
       "      <td>Privado</td>\n",
       "      <td>Física</td>\n",
       "      <td>Femenino</td>\n",
       "      <td>1990.0</td>\n",
       "      <td>ARGENTINA</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2007</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>Buenos Aires</td>\n",
       "      <td>Nacional</td>\n",
       "      <td>1995.0</td>\n",
       "      <td>SEDAN 4 P</td>\n",
       "      <td>RENAULT</td>\n",
       "      <td>RENAULT 19</td>\n",
       "      <td>Privado</td>\n",
       "      <td>Física</td>\n",
       "      <td>Masculino</td>\n",
       "      <td>1986.0</td>\n",
       "      <td>ARGENTINA</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>1995</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   recuperados registro_seccional_provincia automotor_origen  \\\n",
       "0            0                 Buenos Aires         Nacional   \n",
       "1            0                 Buenos Aires         Nacional   \n",
       "2            0                 Buenos Aires         Nacional   \n",
       "\n",
       "   automotor_anio_modelo automotor_tipo_descripcion  \\\n",
       "0                 2000.0                  SEDAN 4 P   \n",
       "1                 2007.0                     FURGON   \n",
       "2                 1995.0                  SEDAN 4 P   \n",
       "\n",
       "  automotor_marca_descripcion automotor_modelo_descripcion  \\\n",
       "0                   CHEVROLET                        CORSA   \n",
       "1                     PEUGEOT                      PARTNER   \n",
       "2                     RENAULT                   RENAULT 19   \n",
       "\n",
       "  automotor_uso_descripcion titular_tipo_persona titular_genero  \\\n",
       "0                   Privado               Física      Masculino   \n",
       "1                   Privado               Física       Femenino   \n",
       "2                   Privado               Física      Masculino   \n",
       "\n",
       "   titular_anio_nacimiento titular_pais_nacimiento  unico_duenio  dia_anio  \\\n",
       "0                   1981.0               ARGENTINA             1        17   \n",
       "1                   1990.0               ARGENTINA             1         3   \n",
       "2                   1986.0               ARGENTINA             1        12   \n",
       "\n",
       "   dia_robo  mes_robo  dia_del_anio  anio_pat  tit_radicado  \n",
       "0         3         1            17      2000             1  \n",
       "1         3         1             3      2007             1  \n",
       "2         5         1            12      1995             1  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finalmente dropeamos las columnas que utilizamos para crear estas columnas aucxiliares\n",
    "data_total.drop(['tramite_fecha', 'fecha_inscripcion_inicial', 'titular_domicilio_provincia'], axis = 1, inplace= True)\n",
    "data_total.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación vamos a generar los dummies correspondientes a las columnas que utilizaremos en los modelos de predicción."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "prov_dummie     = pd.get_dummies(data_total.registro_seccional_provincia)\n",
    "tipo_dummie     = pd.get_dummies(data_total.automotor_tipo_descripcion)\n",
    "marca_dummie    = pd.get_dummies(data_total.automotor_marca_descripcion)\n",
    "modelo_dummie   = pd.get_dummies(data_total.automotor_modelo_descripcion)\n",
    "uso_dummie      = pd.get_dummies(data_total.automotor_uso_descripcion)\n",
    "pais_tit_dummie = pd.get_dummies(data_total.titular_pais_nacimiento)\n",
    "# MARCA NO LA NECESITAMOS, CON MODELO YA TENÉS MARCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correción de algunos dummies\n",
    "tipo_dummie  = tipo_dummie.rename(columns={'PICK UP':'PICK UP tipo'})\n",
    "marca_dummie = marca_dummie.rename(columns={'A.F.F.':'A.F.F. marca', 'PICK UP':'PICK UP marca', 'JEEP':'JEEP marca', 'RENAULT':'RENAULT marca'})\n",
    "#data_total  = data_total.join([prov_dummie, origen_dummie, tipo_dummie,  modelo_dummie, uso_dummie, pais_tit_dummie, marca_dummie]) #marca_dummie,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos dummies para las variables con dos niveles\n",
    "data_total['titular_pers_fisica'] = np.where(data_total['titular_tipo_persona'] == 'Física', 1, 0)\n",
    "data_total['titular_masculino']   = np.where(data_total['titular_genero'] == 'Masculino', 1, 0)\n",
    "data_total['importado']           = np.where(data_total['automotor_origen'] == 'Importado', 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join con las dummies descartando la ultima columna\n",
    "data_total = data_total.join([prov_dummie.iloc[:,:-1], marca_dummie.iloc[:,:-1],\n",
    "                              tipo_dummie.iloc[:,:-1], modelo_dummie.iloc[:,:-1], \n",
    "                              uso_dummie.iloc[:,:-1], pais_tit_dummie.iloc[:,:-1]]) #marca_dummie,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropeamos las columnas a partir de las cuales generamos los dummies\n",
    "data_total.drop(['registro_seccional_provincia', 'automotor_origen', 'automotor_uso_descripcion','automotor_tipo_descripcion',\n",
    "        'automotor_marca_descripcion', 'automotor_modelo_descripcion','titular_pais_nacimiento', \n",
    "       'titular_tipo_persona', 'titular_genero'], axis= 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(62842, 1995)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(data_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparación de los datos para el ajuste del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separación entre x e y\n",
    "x = data_total.drop('recuperados', axis = 1)\n",
    "y = data_total.recuperados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([], dtype=int64), array([], dtype=int64))"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verifico que no haya NANs en x\n",
    "np.where(np.isnan(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La proporción de recuperados en el test es 0.05\n",
      "La proporción de recuperados en el train es 0.05\n"
     ]
    }
   ],
   "source": [
    "xtrain, xtest, ytrain, ytest = train_test_split(x, y, test_size=0.3, random_state=42, stratify=y)\n",
    "print(\"La proporción de recuperados en el test es \"+ str(round(ytest.sum()/np.shape(ytest)[0],2)))\n",
    "print(\"La proporción de recuperados en el train es \"+ str(round(ytrain.sum()/np.shape(ytrain)[0],2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que la proporción de recuperados es, aproximadamente, .05 en el train y el test set. Es decir, la estratificación funcionó bien."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auto-escaleo de las muestras\n",
    "\n",
    "Antes de empezar a trabajar en el balanceo de las muestras, vamos a escalear el xtrain y xtest. De este modo, tanto cuando quitemos muestras como cuando creemos nuevas lo haremos en el espacio normalizado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auto-escaleo de las muestras (mean = 0, std = 1)\n",
    "scaler      = preprocessing.StandardScaler().fit(xtrain)\n",
    "xtrain_scal = scaler.transform(xtrain)  \n",
    "xtest_scal  = scaler.transform(xtest)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Balanceo de la muestra\n",
    "\n",
    "### Undersampling con random\n",
    "\n",
    "Vamos a balancear la muestra como para ver que entendemos el procedimiento, antes de empezar a ajutar cualquier modelo.\n",
    "\n",
    "Lo primero que voy a hacer es probar randowm undersampling, es deir, borrar muestras de la majority al azar. Me quedo con N muestras, que en este caso lo puse como un tercio de los autos no recuperados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mezclamos el dataset\n",
    "shuffled_df = xtrain.sample(frac=1,random_state=4)\n",
    "\n",
    "# Todos los recuperados por un lado\n",
    "recup_df = xtrain.loc[ytrain == 1]\n",
    "recup_df[\"recup\"] = ytrain.loc[ytrain == 1]\n",
    "\n",
    "# Los no recuperados por otro lado\n",
    "no_recup_df = xtrain.loc[ytrain == 0]\n",
    "no_recup_df[\"recup\"] = ytrain.loc[ytrain == 0]\n",
    "\n",
    "# Me quedo con N samples de los no recuperados\n",
    "N = round(no_recup_df.shape[0]/3) # Me quedo con un tercio\n",
    "no_recup_df = no_recup_df.sample(n=N,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "undersampled_xtrain = pd.concat([recup_df, no_recup_df], ignore_index=True)\n",
    "undersampled_ytrain = undersampled_xtrain[\"recup\"]\n",
    "undersampled_xtrain.drop('recup', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La cantidad de muestras originales es 43989\n",
      "La cantidad de muestras después del undersample es 16027\n"
     ]
    }
   ],
   "source": [
    "print(\"La cantidad de muestras originales es \" + str(xtrain.shape[0]))\n",
    "print(\"La cantidad de muestras después del undersample es \" + str(undersampled_xtrain.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oversampling con SMOTE\n",
    "\n",
    "SMOTE me permite aumentar el número de muestras de la minoría hasta una cierta proporción de la mayoría (dada en el código por *prop*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "prop = .5 # Es decir, la minería va a ser prop veces la mayoría\n",
    "\n",
    "# Resampleo la minoría\n",
    "sm = SMOTE(sampling_strategy=prop, random_state=7)\n",
    "\n",
    "# Fiteo el modelo para generar la nueva data sintética\n",
    "balanced_xtrain, balanced_ytrain = sm.fit_sample(undersampled_xtrain, undersampled_ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La cantidad de muestras originales (despues del undersampling) es 16027\n",
      "La cantidad de muestras aumentadas es 20971\n"
     ]
    }
   ],
   "source": [
    "print(\"La cantidad de muestras originales (después del undersampling) es \" + str(undersampled_xtrain.shape[0]))\n",
    "print(\"La cantidad de muestras aumentadas es \" + str(balanced_xtrain.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La cantidad de recuperados originales (después del undersampling) es 2046\n",
      "La cantidad de recuperados aumentados es 6990\n"
     ]
    }
   ],
   "source": [
    "print(\"La cantidad de recuperados originales (después del undersampling) es \" + str(undersampled_ytrain.sum()))\n",
    "print(\"La cantidad de recuperados aumentados es \" + str(balanced_ytrain.sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La proporción de recuperados en la muestra aumentada es 0.33331743836726907\n"
     ]
    }
   ],
   "source": [
    "print(\"La proporción de recuperados en la muestra aumentada es \" + \n",
    "      str(balanced_ytrain.sum()/balanced_xtrain.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De esta forma logramos tener que 1 de cada 3 muestras sea de un auto recuperado.\n",
    "\n",
    "Ahora que comprendemos como funcione el balanceo (random oversampling y SMOTe), tenemos que decidir cómo elegir la mejor combinación de parámetros para nuestro modelo. Como tenemos recursos (hardware y tiempo) limitados, vamos a hacer un barrido enre algunos posibles valores de los parámetros y vamos a ajustar un KNN para ver cual combinación mejora más la performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN\n",
    "\n",
    "Vamos a ajustar un KNN (con K de 1 a 10) para los datos originales y para versiones del dataset modificado modificando dos parámetros: la cantidad de muestras extraídas en el undersampling y la cantidad de muestras agregadas en el oversampling. Entre todas estas versiones del modelo nos vamos a quedar con el que maximice el Recall (una medida más confiable para muestras no balanceadas).\n",
    "\n",
    "Para el undersampling nos vamos a quedar con el 25%, 50% y 75% de los autos no recuperados originales, y para cada uno de esos datasets vamos a hacer un SMOTe que iguales las proporciones de recuperados y no recuperados (*prop = 1*) o en el que la proporción de recuperados sea 0.5 (*prop =0.5*)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN con los datos originales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, recall_score\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creo un vector para guardar en recall del original\n",
    "original_recall_knn = np.zeros(np.shape(Ks)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Corro el KNN con los datos originales\n",
    "for r in Ks:\n",
    "    # entreno un KNN classifier con \"k=r+1\"\n",
    "    neigh = KNeighborsClassifier(n_neighbors=r)\n",
    "    neigh.fit(xtrain,ytrain)\n",
    "    # calculo la prediccion\n",
    "    ypred_knn = neigh.predict(xtest)\n",
    "    # guardo el resultado de prediccion en la posicion r del vector \"acc_knn\"\n",
    "    original_recall_knn[r-1] = recall_score(ytest, ypred_knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veamos el recall en función de K para los datos originales\n",
    "customPalette = ['#630C3A', '#39C8C6', '#D3500C', '#FFB139']\n",
    "plt.figure(figsize=(9,6))\n",
    "plt.xlabel('#Ks')\n",
    "plt.ylabel('Recall')\n",
    "plt.title('Recall con KNN de los datos originales')\n",
    "plt.plot(Ks,original_recall_knn,color=customPalette[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cuál es el mejor recall en el KNN con los datos originales\n",
    "max_recall = max(original_recall_knn)\n",
    "max_K = Ks[np.argmax(original_recall_knn)]\n",
    "print(\"El recall más grande es con \" + str(round(max_recall,2))+ ' con '+ \n",
    "      str(max_K)+ ' neighbours')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En los datos originales el mayor recall da para k=2.\n",
    "\n",
    "A continuación vamos a evaluar como se modifica el recall para las opciones de oversampling y undersampling antes descriptas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dejamos fiteado el mejor KNN\n",
    "best_KNN = KNeighborsClassifier(n_neighbors=max_K)\n",
    "best_KNN.fit(xtrain,ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificamos la performance con el ytest (vemos la matriz de confusión)\n",
    "ypred_knn = best_KNN.predict(xtest)\n",
    "cm_knn  = confusion_matrix(ytest, ypred_knn_bal)\n",
    "print(cm_knn_bal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('El recall obtenido con el mejor KNN y el dataset original es de:',\n",
    "      + round(recall_score(ytest, ypred_rf),2))\n",
    "print('La accuracy obtenido con el mejor KNN y el dataset original es de:',\n",
    "      + round(accuracy_score(ytest, ypred_rf),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como es de esperarse para datos desbalanceados, el accuracy es alto pero el recall es bajo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN con los datos balanceados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creo las variables en las que voy a guardas los recalls\n",
    "\n",
    "# Indicamos la cantidad de valores de balanceo que queremos probar\n",
    "#      Undersampling\n",
    "n_norecup = xtrain.loc[ytrain == 0].shape[0] # Cantidad de autos no recuperados\n",
    "params_under = [n_norecup*.75, n_norecup*.5, n_norecup*.25] # Nos quedamos con 75%, 50% o 25% de \n",
    "                                                            # los no recuperados\n",
    "#      Overrsampling\n",
    "params_smote = [.5, 1] # Hacemos que los recuperados sean la mitad o la misma cantidad que los no\n",
    "                      # recuperados\n",
    "    \n",
    "# Indicamos la cantidad de valores de K que queremos probar para cada combinación anterior\n",
    "Ks = range(1,10)\n",
    "\n",
    "# Creo un vector para guardar en recall para los datos balanceados\n",
    "balanced_recall_knn = np.zeros([ np.shape(params_under)[0],  \n",
    "                                 np.shape(params_smote)[0], \n",
    "                                 np.shape(Ks)[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ahora voy a preprocesar y correr el KNN para cada combinación de \n",
    "# parámetros de under y oversampling\n",
    "for idx_under in range(0,np.shape(params_under)[0]):\n",
    "    for idx_smote in range(0,np.shape(params_smote)[0]):\n",
    "        for r in Ks:\n",
    "        \n",
    "            ### UNDERSAMPLING ###\n",
    "\n",
    "            # Mezclamos el dataset\n",
    "            shuffled_df = xtrain.sample(frac=1,random_state=4)\n",
    "\n",
    "            # Todos los recuperados por un lado\n",
    "            recup_df = xtrain.loc[ytrain == 1]\n",
    "            recup_df[\"recup\"] = ytrain.loc[ytrain == 1]\n",
    "\n",
    "            # Los no recuperados por otro lado\n",
    "            no_recup_df = xtrain.loc[ytrain == 0]\n",
    "            no_recup_df[\"recup\"] = ytrain.loc[ytrain == 0]\n",
    "\n",
    "            # Me quedo con N samples de los no recuperados\n",
    "            no_recup_df = no_recup_df.sample(n=round(params_under[idx_under]),random_state=42)\n",
    "\n",
    "            # Armo el x_train e y_train undersampleados\n",
    "            undersampled_xtrain = pd.concat([recup_df, no_recup_df], ignore_index=True)\n",
    "            undersampled_ytrain = undersampled_xtrain[\"recup\"]\n",
    "            undersampled_xtrain.drop('recup', axis=1, inplace=True)\n",
    "\n",
    "            ### SMOTE ###\n",
    "\n",
    "            # Creo el objeto de SMOTE\n",
    "            sm = SMOTE(sampling_strategy=params_smote[idx_smote], random_state=42)\n",
    "\n",
    "            # Fiteo el modelo para generar la nueva data sintética\n",
    "            balanced_xtrain, balanced_ytrain = sm.fit_sample(undersampled_xtrain, undersampled_ytrain)\n",
    "\n",
    "            ### KNN ###\n",
    "\n",
    "            # entreno un KNN classifier con \"k=r+1\"\n",
    "            neigh = KNeighborsClassifier(n_neighbors=r)\n",
    "            neigh.fit(balanced_xtrain,balanced_ytrain)\n",
    "            # calculo la prediccion\n",
    "            ypred_knn = neigh.predict(xtest)\n",
    "            # guardo el resultado de prediccion en la posicion r del vector \"acc_knn\"\n",
    "            balanced_recall_knn[idx_under, idx_smote, r-1] = recall_score(ytest, ypred_knn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veamos el recall en función de K para los datos balanceados\n",
    "markers = ['s', 'o']\n",
    "plt.figure(figsize=(12,9))\n",
    "plt.xlabel('#Ks')\n",
    "plt.ylabel('Recall')\n",
    "plt.title('Recall con KNN de los datos balanceados')\n",
    "for idx_under in range(0,np.shape(params_under)[0]):\n",
    "    for idx_smote in range(0,np.shape(params_smote)[0]):\n",
    "        plt.plot(Ks, balanced_recall_knn[idx_under, idx_smote, :],\n",
    "                 color=customPalette[idx_under])\n",
    "        plt.scatter(Ks, balanced_recall_knn[idx_under, idx_smote, :],\n",
    "                    color=customPalette[idx_under], marker=markers[idx_smote],\n",
    "                    label='Under:'+ str(params_under[idx_under])+ ' SMOTe:' + str(params_smote[idx_smote]))\n",
    "plt.legend()\n",
    "plt.savefig('./figs/Recalls_SMOTe_KNN.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cuál es el mejor recall en el KNN con los datos balanceados\n",
    "max_recall = np.amax(balanced_recall_knn)\n",
    "max_under  = params_under[np.where(balanced_recall_knn == np.amax(balanced_recall_knn))[0][0]]\n",
    "max_smote  = params_smote[np.where(balanced_recall_knn == np.amax(balanced_recall_knn))[1][0]]\n",
    "max_K      = Ks[np.where(balanced_recall_knn == np.amax(balanced_recall_knn))[2][0]]\n",
    "\n",
    "print(\"El recall más grande es con \" + str(round(max_recall,2))+ \n",
    "      ' con '+ str(max_K)+ ' neighbours'+ \n",
    "      ', undersamplig a '+ str(max_under) + ' muestras'+\n",
    "      ' y con una proporcion de SMOTE de '+ str(max_smote))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como vemos que no llega a converger en número de Ks, vamos a tomar las opciones de SMOTe y undersampling con mejor performance y extender la simulación a K=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Primero creamos el balanced data con el mejor Fit\n",
    "\n",
    "### UNDERSAMPLING ###\n",
    "\n",
    "# Mezclamos el dataset\n",
    "shuffled_df = xtrain.sample(frac=1,random_state=4)\n",
    "\n",
    "# Todos los recuperados por un lado\n",
    "recup_df = xtrain.loc[ytrain == 1]\n",
    "recup_df[\"recup\"] = ytrain.loc[ytrain == 1]\n",
    "\n",
    "# Los no recuperados por otro lado\n",
    "no_recup_df = xtrain.loc[ytrain == 0]\n",
    "no_recup_df[\"recup\"] = ytrain.loc[ytrain == 0]\n",
    "\n",
    "# Me quedo con N samples de los no recuperados\n",
    "no_recup_df = no_recup_df.sample(n=round(max_under),random_state=42)\n",
    "\n",
    "# Armo el x_train e y_train undersampleados\n",
    "undersampled_xtrain = pd.concat([recup_df, no_recup_df], ignore_index=True)\n",
    "undersampled_ytrain = undersampled_xtrain[\"recup\"]\n",
    "undersampled_xtrain.drop('recup', axis=1, inplace=True)\n",
    "\n",
    "### SMOTE ###\n",
    "\n",
    "# Creo el objeto de SMOTE\n",
    "sm = SMOTE(sampling_strategy=max_smote, random_state=42)\n",
    "\n",
    "# Fiteo el modelo para generar la nueva data sintética\n",
    "balanced_xtrain, balanced_ytrain = sm.fit_sample(undersampled_xtrain, undersampled_ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y ahora vamos a correr un loop en KNN\n",
    "Ks = np.arange(10,110,10)\n",
    "best_recall_knn = np.zeros(np.shape(Ks)[0])\n",
    "for r in range(0, np.shape(Ks)[0]):\n",
    "    # entreno un KNN classifier con \"k=r+1\"\n",
    "    neigh = KNeighborsClassifier(n_neighbors=Ks[r])\n",
    "    neigh.fit(balanced_xtrain,balanced_ytrain)\n",
    "    # calculo la prediccion\n",
    "    ypred_knn = neigh.predict(xtest)\n",
    "    # guardo el resultado de prediccion en la posicion r del vector \"acc_knn\"\n",
    "    best_recall_knn[r-1] = recall_score(ytest, ypred_knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veamos el recall en función de K el best balanceado\n",
    "plt.figure(figsize=(9,6))\n",
    "plt.xlabel('#Ks')\n",
    "plt.ylabel('Recall')\n",
    "plt.title('Recall con KNN de los mejores undersampling y SMOTe')\n",
    "plt.plot(Ks,best_recall_knn,color=customPalette[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como parece que aumentando mucho el K o hay un gran aumento del recall, nos vamos a quedar ocnconverger a un valor de recall razonable, nos vamos a quedar con los *max_under*, *max_smote* y *max_K* definidos anteriormente para lo que sigue del ajuste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dejamos fiteado el mejor KNN\n",
    "best_KNN_bal = KNeighborsClassifier(n_neighbors=max_K)\n",
    "best_KNN_bal.fit(balanced_xtrain,balanced_ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificamos la performance con el ytest (vemos la matriz de confusión)\n",
    "ypred_knn_bal = best_KNN_bal.predict(xtest)\n",
    "cm_knn_bal = confusion_matrix(ytest, ypred_knn_bal)\n",
    "print(cm_knn_bal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('El recall obtenido con el mejor KNN y el dataset balanceado es de:',\n",
    "      + round(recall_score(ytest, ypred_knn_bal),2))\n",
    "print('La accuracy obtenido con el mejor KNN y el dataset balanceado es de:',\n",
    "      + round(accuracy_score(ytest, ypred_knn_bal),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random forest con los datos originales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hacemos un Random Forest con los datos originales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos el objeto Random Forest\n",
    "rfc=RandomForestClassifier(random_state=42)\n",
    "# Hacemos un gridsearch\n",
    "param_grid = { \n",
    "    'n_estimators': [20,50],\n",
    "    'max_features': ['sqrt', 'log2'],\n",
    "    'max_depth'   : [40],\n",
    "    'criterion'   : ['gini']\n",
    "}\n",
    "CV_rfc = GridSearchCV(estimator=rfc, param_grid=param_grid, cv= 5)\n",
    "CV_rfc.fit(xtrain, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Los mejores parámetros del Random Forest son\n",
    "CV_rfc.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finalmente ajusatmos le mejor modelo\n",
    "rfc1=RandomForestClassifier(random_state = 42, \n",
    "                            max_features = CV_rfc.best_params_['max_features'], \n",
    "                            n_estimators = CV_rfc.best_params_['n_estimators'], \n",
    "                            max_depth    = CV_rfc.best_params_['max_depth'], \n",
    "                            criterion    = CV_rfc.best_params_['criterion'])\n",
    "rfc1.fit(xtrain, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificamos la performance con el ytest (vemos la matriz de confusión)\n",
    "ypred_rf = CV_rfc.predict(xtest)\n",
    "cm_rf = confusion_matrix(ytest, ypred_rf)\n",
    "print(cm_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('El recall obtenido con el mejor Random Forest y el dataset balanceado es de:',\n",
    "      + round(recall_score(ytest, ypred_rf),2))\n",
    "print('La accuracy obtenido con el mejor Random Forest y el dataset balanceado es de:',\n",
    "      + round(accuracy_score(ytest, ypred_rf),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como es de esperarse, el accuracy da muy bien pero el recall da muy mal (algo típico en datasets dsbalanceados)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random forest con los datos balanceados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora vamos a ajustar un Random Forest con los datos balanceados (el mejor balanceo) obtenido anteriormente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos el objeto Random Forest\n",
    "rfc=RandomForestClassifier(random_state=42)\n",
    "# Hacemos un gridsearch\n",
    "param_grid = { \n",
    "    'n_estimators': [20,50],\n",
    "    'max_features': ['sqrt', 'log2'],\n",
    "    'max_depth'   : [40],\n",
    "    'criterion'   : ['gini']\n",
    "}\n",
    "CV_rfc = GridSearchCV(estimator=rfc, param_grid=param_grid, cv= 5)\n",
    "CV_rfc.fit(balanced_xtrain, balanced_ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Los mejores parámetros del Random Forest son\n",
    "CV_rfc.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finalmente ajusatmos le mejor modelo\n",
    "rfc1=RandomForestClassifier(random_state = 42, \n",
    "                            max_features = CV_rfc.best_params_['max_features'], \n",
    "                            n_estimators = CV_rfc.best_params_['n_estimators'], \n",
    "                            max_depth    = CV_rfc.best_params_['max_depth'], \n",
    "                            criterion    = CV_rfc.best_params_['criterion'])\n",
    "rfc1.fit(oversampled_xtrain, oversampled_ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificamos la performance con el ytest (vemos la matriz de confusión)\n",
    "ypred_rf_bal = CV_rfc.predict(xtest)\n",
    "cm_rf_bal = confusion_matrix(ytest, ypred_rf_bal)\n",
    "print(cm_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('El recall obtenido con el mejor Random Forest y el dataset balanceado es de:',\n",
    "      + round(recall_score(ytest, ypred_rf_bal),2))\n",
    "print('La accuracy obtenido con el mejor KNN y el dataset balanceado es de:',\n",
    "      + round(accuracy_score(ytest, ypred_rf_bal),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El recall da peor que en el KNN pero el accuracy da mucho mejor. Habría que analizar qué significa esto para nuestro problema."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest con PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a probar hacer un Random Forest con el dataset balanceado. Para esto empezamos ajustando un PCA con el dataset balanceado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "# Cantidad de componentes a extraer\n",
    "n_comps = 5\n",
    "# Definimos PCA\n",
    "pca = PCA(n_components= n_comps)\n",
    "# Fit_transform del PCA a nuestros datos\n",
    "xpca = pd.DataFrame(pca.fit_transform(balanced_xtrain))\n",
    "# Obtenemos los auto-valores\n",
    "eigenvalues = pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Una figura que muestra la varianza explicada por cada componente\n",
    "components = range(1,n_comps + 1)\n",
    "plt.bar(components,eigenvalues)\n",
    "plt.xticks(components)\n",
    "plt.title('Explained variance of top 5 principal components')\n",
    "plt.xlabel('Top 20 Principal Components')\n",
    "plt.ylabel('Explained Variance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora sí vamos a ajustar el Random Forest y ver cuánto cambia la performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest con PCA\n",
    "rfc=RandomForestClassifier(random_state=42)\n",
    "param_grid = { \n",
    "    'n_estimators': [20,50],\n",
    "    'max_features': ['sqrt', 'log2'],\n",
    "    'max_depth'   : [40],\n",
    "    'criterion'   : ['gini']\n",
    "}\n",
    "CV_rfc = GridSearchCV(estimator=rfc, param_grid=param_grid, cv= 5)\n",
    "CV_rfc.fit(xpca, balanced_ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tranformamos el xtest al espacio del PCA\n",
    "xpca_test = pd.DataFrame(pca.fit_transform(xtest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificamos la performance con el ytest (vemos la matriz de confusión)\n",
    "ypred_rf_pca = CV_rfc.predict(xpca_test)\n",
    "cm_rf_pca = confusion_matrix(ytest, ypred_rf_pca)\n",
    "print(cm_rf_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('El recall obtenido con el mejor Random Forest, PCA y el dataset balanceado es de:',\n",
    "      + round(recall_score(ytest, ypred_rf_pca),2))\n",
    "print('La accuracy obtenido con el mejor Random Forest, PCA y el dataset balanceado es de:',\n",
    "      + round(accuracy_score(ytest, ypred_rf_pca),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso sube el recall de nuevo pero con el costo de bajar también el accuracy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
